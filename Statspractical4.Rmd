---
title: A Statistical Examination of Socioeconomic Patterns in the Brexit Referendum
  Outcome
author: "Arunthavaraj Ananthavel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    includes:
      in_header: header.tex
      latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In June 2016, the United Kingdom held a poll on whether to leave or
remain in the European Union, dubbed the "Brexit" vote. Surprisingly,
the Leave campaign won a victory, contradicting many experts'
expectations of a Remain win. After the referendum, analysts tried to
understand patterns and demographic factors that could explain how
different areas and region voted.

The Guardian newspaper presented one such analysis, showing apparent
trends in how areas with different resident ages, incomes, education
levels, and social classes voted in the polling. While the Guardian's
analysis was mainly visual observations, this report aims to conduct
more rigorous statistical analysis on the same data-set. The goal is to
identify and quantify the key factors influencing the Brexit Leave or
Remain votes.

# Methodology:

To analyze patterns in the Brexit voting data using K-means clustering,
the data will be prepared by scaling it appropriately. The optimal
number of clusters will be determined using the Elbow and Silhouette
methods. K-means clustering will be applied using the optimal number of
clusters. The clusters will be analyzed and validated by checking their
sizes and how well they separate demographic groups based on the Brexit
vote. The clusters will be visualized after performing PCA. A check will be made if
certain clusters are associated with voting behaviors.

For logistic regression analysis, a logistic regression model will be
fit using all available predictors. The coefficients will be
interpreted, their significance, and the direction and strength of their
effects. The results will be compared with patterns reported by The
Guardian. The model performance will be validated using methods like ROC
curve analysis and confusion matrix.

To evaluate interpretability and variability, factors like
multicollinearity, outliers, model fit, and variable scales that
influence interpretability will be discussed. Bootstrap aggregation will
be used to assess the stability and variability of the logistic
regression coefficients.

Finally, an alternative approach like Lasso(L1) or Ridge(L2) regression
Models will be proposed and implemented. It will be compared to standard
logistic regression in terms of coefficient stability, model accuracy,
and interpretability.

# Data Loading and Inspection

The data-set from the Guardian contains 6 attributes per electoral ward
area:

```{r,include=FALSE, echo=FALSE}
library(knitr)
library(rpart)
library(cluster)
library(GGally)
library(ggplot2)
library(gridExtra)
library(tidyverse)
library(glmnet)

```

```{r datatable, include=TRUE, echo=FALSE}
# Create a dataframe for the table content
data_info <- data.frame(
  Variable = c("notBornUK", "medianIncome", "medianAge", "withHigherEd", "abc1"),
  Description = c("Proportion of residents born outside the UK",
                  "Median resident income",
                  "Median resident age",
                  "Proportion with university education",
                  "Proportion in middle to upper social classes")
)

knit_table <- kable(data_info, "markdown", align = 'l', caption = "Key Columns in Dataset")

# Print the knitted table
knit_table
```

The output voteBrexit indicates if an area had a majority Leave (TRUE)
or Remain (FALSE) vote. Initial visualizations like scatter plots and
box plots can reveal apparent patterns between the input variables and
the Brexit vote outcome. [Insert relevant visualizations and
observations.]

However, visualizations alone cannot fully capture complex, non-linear
relationships between multiple variables.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.align='center',fig.height=4,fig.width=4}
# Read the CSV data
# Load required packages
# Read the CSV data
data <- read.csv("brexit.csv")

# Install and load required packages
library(ggplot2)
library(scales)
library(dplyr)

# Convert voteBrexit to factor with labels "remain" and "leave"
data$voteBrexit <- factor(data$voteBrexit, levels = c(FALSE, TRUE), labels = c("remain", "leave"))

# Calculate percentages
remain_percent <- round(sum(data$voteBrexit == "remain") / nrow(data) * 100, 2)
leave_percent <- round(sum(data$voteBrexit == "leave") / nrow(data) * 100, 2)

# Create data frame for pie chart
pie_data <- data.frame(
  category = c("Remain", "Leave"),
  value = c(remain_percent, leave_percent)
)

# Create pie chart
ggplot(pie_data, aes(x = "", y = value, fill = category)) +
  geom_col(width = 1) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("skyblue", "goldenrod")) +
  labs(title = "Vote Brexit: Remain vs Leave") +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    plot.title = element_text(hjust = 0.5),
    legend.position = "right"
  ) +
  geom_text(aes(label = paste0(value, "%")), position = position_stack(vjust = 0.5))

```

```{r initial, include=TRUE, echo=FALSE}
# Load the data
brexit_data <- read.csv("brexit.csv")
```

```{r,include=TRUE, echo=FALSE}
head(brexit_data)
```

```{r,include=TRUE,echo=FALSE,fig.align='center'}
summary(brexit_data)
```

# Preliminary analysis:

1.  Social Class Distribution (`abc1`):\
    The population is skewed towards the middle to upper social classes,
    with a median around 42%. This suggests a stable economic status for
    a large portion of residents.

2.  Immigration Status (`notBornUK`):\
    The median of residents born outside the UK is around 9.24%. This
    shows a notable presence of foreign-born individuals.

3.  Income Distribution (`medianIncome`):\
    The median resident income is around 25.77%, suggesting a moderate
    income level. However, there is variability in income levels, as
    shown by the range and positive skew.

4.  Age Distribution (`medianAge`):\
    The median resident age is around 54.55%, pointing to an older
    population. The positive skew suggests a larger portion of older
    individuals compared to younger ones.

5.  Educational Attainment (`withHigherEd`):\
    A median of 28.95% have a university education, indicating a
    significant portion pursued higher education. This may influence
    workforce skill level and socioeconomic mobility.

6.  Brexit Voting Pattern (`voteBrexit`):\
    With 237 observations for "voted for Brexit" and 107 for "did not
    vote for Brexit," a notable portion supported Brexit, which could
    have political and economic effects.

# Data Cleaning and Visualization:

### Data Cleaning:

The data look clean with no missing or 'NA' values. So, the data-set can
be ready used for the modelling.

```{r dataclean}
sum(is.na(brexit_data))
```

### Data Visualization:

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.align='center'}

# Load the data
data <- read.csv("brexit.csv")

# Create histograms for each numerical variable with a vertical line representing the mean
p1 <- ggplot(data, aes(x=abc1)) + 
  geom_histogram(bins=30, fill="blue", color="black") + 
  geom_vline(aes(xintercept=mean(abc1)), color="red", linetype="dashed", size=1) +
  ggtitle("Histogram of abc1")

p2 <- ggplot(data, aes(x=notBornUK)) + 
  geom_histogram(bins=30, fill="green", color="black") + 
  geom_vline(aes(xintercept=mean(notBornUK)), color="red", linetype="dashed", size=1) +
  ggtitle("Histogram of notBornUK")

p3 <- ggplot(data, aes(x=medianIncome)) + 
  geom_histogram(bins=30, fill="red", color="black") + 
  geom_vline(aes(xintercept=mean(medianIncome)), color="red", linetype="dashed", size=1) +
  ggtitle("Histogram of medianIncome")

p4 <- ggplot(data, aes(x=medianAge)) + 
  geom_histogram(bins=30, fill="purple", color="black") + 
  geom_vline(aes(xintercept=mean(medianAge)), color="red", linetype="dashed", size=1) +
  ggtitle("Histogram of medianAge")

p5 <- ggplot(data, aes(x=withHigherEd)) + 
  geom_histogram(bins=30, fill="orange", color="black") + 
  geom_vline(aes(xintercept=mean(withHigherEd)), color="red", linetype="dashed", size=1) +
  ggtitle("Histogram of withHigherEd")

# Arrange plots in a grid
grid.arrange(p1, p2, p3, p4, p5, ncol=2)


```


The histogram of abc1 appears bi-modal, suggesting two predominant
groups. The histogram of `notBornUK` is highly skewed right, indicating
most data points have low values, and areas with high proportion of
non-UK-born residents are uncommon. The histogram of `medianIncome` is
roughly normal but slightly skewed right, suggesting income disparity
across locations. The histogram of `medianAge` is fairly uniform with
several peaks, indicating diverse age demographics across regions. The
histogram of `withHigherEd` shows a right-skewed distribution, suggesting
most areas have a lower percentage of people with higher education, but
a few areas have a high percentage.

These distributions give a broad picture of demographic and
socio-economic conditions across regions. The skewness in variables like
`notBornUK` and `withHigherEd` suggests disparities in birth origins of
residents and educational attainment. The data might be used to explore
how these factors correlate with economic conditions, age demographics,
and political preferences like attitudes towards Brexit.

```{r viz,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.align='center',fig.height=9,fig.width=14}
brexit_data_scaled <- scale(brexit_data[, 1:5])
set.seed(123)  # for reproducibility
optimal_clusters <- kmeans(brexit_data_scaled, centers = 2, nstart = 50)

# Assigning cluster results back to the data
brexit_data$cluster <- as.factor(optimal_clusters$cluster)  # Convert cluster results to factor for coloring

# Plotting clusters using GGally's ggpairs function, enhanced with ggplot2 for better visual distinction
ggpairs(brexit_data, columns = 1:5, mapping = ggplot2::aes(color = cluster, shape = cluster)) +
  ggplot2::theme_bw() +  # Use a clean theme
  ggplot2::labs(color = "Cluster", shape = "Cluster") +  # Label legends
  ggplot2::theme(legend.position = "right",  # Adjust legend position
                 plot.title = element_text(hjust = 0.5),  # Center the plot title
                 axis.text = element_text(size = 12),  # Enhance text size for clarity
                 axis.title = element_text(size = 14)) +  # Enhance axis title size
  ggplot2::ggtitle("Cluster Visualization of Brexit Data")  # Add a title to the plot
```

The provided plot is a pairs plot, which includes scatter plots for each
pair of variables along with density plots for each variable. The
diagonal plots show the distribution of each variable, with different
colors likely representing different clusters. The lower triangle
scatter plots display the relationship between two variables, with
points colored by cluster. The presence of distinct colors indicates the
data is grouped into clusters. The patterns in these plots suggest
correlations or relationships between the variables.

Each cell above the diagonal provides the correlation coefficient
between the pair of variables, along with significance levels. The
coefficients indicate how strongly the variables are related. Some cells
contain additional statistics for each cluster, suggesting how the
variables differ by cluster or how relationships vary within clusters.

The significant differences in distributions and relationships by
cluster suggest the clustering found meaningful groups within the data.
Given these variables include demographic and socio-economic indicators,
their relationships and distributions can help understand factors
influencing the Brexit vote. Variables with strong correlations or
distinct clustering might be powerful predictors in modeling the vote
outcome.

The detailed visualization aids in identifying informative variables and
how they interact, crucial for modeling and interpreting impact of
factors on the Brexit decision. The cluster-specific insights provide a
nuanced understanding that can refine predictive models or guide
targeted analyses.

## Data Scaling:

Scaling ensures all variables contribute equally to the analysis and
allows for meaningful comparisons between variables. It improves model
performance for algorithms assuming data is centered and has standard
deviation of 1. The code scales the first five columns of the data-set,
transforming data so each variable has mean 0 and standard deviation 1.
It also selects specific columns and scales them, storing the result in
the inputs variable. This standardizes values, making them suitable for
analysis requiring standardized input data.

```{r standardise code, include=TRUE, echo=FALSE}
brexit_data_scaled <- scale(brexit_data[, 1:5])

```

# Data Modeling:

## Task 1: K-Means Clustering:

K-means clustering is well-suited for analyzing the Brexit data-set due
to its simplicity, efficiency in handling large datasets, and ability to
identify groupings based on quantitative socio-economic variables like
social class, income, age, education, and proportion not born in the UK.
A key advantage is its capacity to reveal patterns showing how these
demographics correlate with Brexit voting tendencies across electoral
wards.

Despite limitations like sensitivity to outliers and requiring the
number of clusters to be specified, K-means remains a robust tool for
exploratory analysis of the Brexit data. Its ability to uncover
underlying patterns in socio-economic and demographic factors makes it
valuable for understanding the drivers behind the referendum's voting
outcomes.

```{r optimal k using silhoutte,include=TRUE, echo=FALSE}
inputs <- scale(brexit_data[, c("abc1", "medianIncome", "medianAge", "withHigherEd", "notBornUK")])
# Calculate optimal k using Silhouette method before PCA
sil_widths_pre_pca <- sapply(2:10, function(k) {
  km <- kmeans(inputs, centers=k, nstart=25)
  avg_silhouette <- mean(silhouette(km$cluster, dist(inputs))[, 3])
  return(avg_silhouette)
})

optimal_k_pre_pca <- which.max(sil_widths_pre_pca) + 1
```

The optimal number of clusters k=2 was determined through the Silhouette
score method, which measures the similarity of data points within
clusters compared to other clusters. k=2 provided the highest average
Silhouette score, suggesting strong cohesion and separation of the two
clusters. With the Brexit vote being a binary outcome (Leave or Remain),
it is logical that the data-set might naturally split into two groups
reflecting these voting patterns. Initial exploratory data analysis may
also support a bifurcation based on socio-economic or demographic
factors aligning with the two voting patterns.

## Performing PCA analysis:

```{r PCA,include=TRUE, echo=FALSE}
# Perform PCA
pca_result <- prcomp(inputs, scale. = TRUE)
pca_data <- data.frame(pca_result$x[, 1:2])  # using the first two principal components

# Calculate optimal k using Silhouette method after PCA
sil_widths_post_pca <- sapply(2:10, function(k) {
  km <- kmeans(pca_data, centers=k, nstart=25)
  avg_silhouette <- mean(silhouette(km$cluster, dist(pca_data))[, 3])
  return(avg_silhouette)
})

optimal_k_post_pca <- which.max(sil_widths_post_pca) + 1

# Print optimal k values
print(paste("Optimal k before PCA:", optimal_k_pre_pca))
print(paste("Optimal k after PCA:", optimal_k_post_pca))

```

The PCA analysis was conducted to verify the k value about selecting the
principal components. Visualizing the clusters using PCA-transformed
data can confirm the clusters are visually distinct, supporting the
clustering outcome. Examining the relationship between the clusters and
the actual Brexit vote outcomes can assess if the clusters are
predictive or associated with the vote results. The choice of k=2 is
justified by statistical measures and practical considerations related
to the binary nature of the Brexit vote. Visual and analytical
assessments of the clusters in relation to vote outcomes further support
this choice, potentially revealing underlying bases for voting patterns.
Such analysis can help understand critical factors influencing voting
behavior in the Brexit referendum, making k=2 statistically valid and
contextually meaningful.

```{r kmeans,include=TRUE, echo=FALSE,fig.align='center',fig.height=4}
# Perform K-means clustering on PCA results with the optimal number of clusters
set.seed(123)
final_kmeans <- kmeans(pca_data, centers=optimal_k_post_pca, nstart=25)
brexit_data$cluster <- final_kmeans$cluster

# Plotting the silhouette scores to visualize the choice of number of clusters before and after PCA
par(mfrow=c(1, 2))  # setting layout for 2 plots
plot(2:10, sil_widths_pre_pca, type='b', col='blue', xlab="Number of clusters", ylab="Average silhouette width",
     main="Pre-PCA Silhouette Scores")
plot(2:10, sil_widths_post_pca, type='b', col='red', xlab="Number of clusters", ylab="Average silhouette width",
     main="Post-PCA Silhouette Scores")

```



```{r,include=TRUE, echo=FALSE,fig.align='center',fig.width=16,fig.height=6,warning=FALSE}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(gridExtra)

# Perform PCA
pca_result <- prcomp(inputs, scale. = TRUE)
pca_data <- data.frame(pca_result$x[, 1:2])  # using the first two principal components

# Calculate optimal k using Silhouette method after PCA
sil_widths_post_pca <- sapply(2:10, function(k) {
  km <- kmeans(pca_data, centers = k, nstart = 25)
  avg_silhouette <- mean(silhouette(km$cluster, dist(pca_data))[, 3])
  return(avg_silhouette)
})

optimal_k_post_pca <- which.max(sil_widths_post_pca) + 1

# Perform K-means clustering on PCA results with the optimal number of clusters
set.seed(123)
final_kmeans <- kmeans(pca_data, centers = optimal_k_post_pca, nstart = 25)
pca_data$cluster <- as.factor(final_kmeans$cluster)

# Function to create a convex hull for each cluster
find_hull <- function(df) df[chull(df$PC1, df$PC2), ]

hulls <- pca_data %>%
  group_by(cluster) %>%
  do(find_hull(.))

# Plot 1: Scatter plot with convex hulls
plot1 <- ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 3) +
  geom_polygon(data = hulls, aes(x = PC1, y = PC2, fill = cluster), color = "black", alpha = 0.2) +
  labs(title = "K-means Clusters on PCA Results", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster") +
  scale_fill_discrete(name = "Cluster", guide = FALSE)

# Add PCA components to brexit_data
brexit_data$PC1 <- pca_result$x[, 1]
brexit_data$PC2 <- pca_result$x[, 2]

# Plot 2: PCA-based Clustering of Brexit Data
plot2 <- ggplot(brexit_data, aes(x = PC1, y = PC2, color = as.factor(cluster))) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("red", "blue")) +
  labs(title = "PCA-based Clustering of Brexit Data", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")

# Arrange the plots in a grid
grid.arrange(plot2, plot1, ncol = 2)
```

```{r codead,include=TRUE, echo=FALSE}
table(brexit_data$cluster, brexit_data$voteBrexit)
```

The clustering results associated with Brexit vote outcomes indicate how
clusters relate to voting behavior. The table shows:

Cluster 1: 55 wards voted Remain, 14 voted Leave.\
Cluster 2: 52 wards voted Remain, 223 voted Leave.

### Interpretation:

Cluster 2 strongly correlates with Leave vote, indicating
characteristics defining this cluster are associated with Brexit
preference.

Cluster 1 captures different characteristics, with regions leaning more
towards staying in EU.

The clustering separates wards into two groups with distinct voting
behaviors. This implies input variables like income, age, education
level could be significant indicators of Brexit voting preference.

### Chi-squared test:

```{r chisq,include=TRUE, echo=FALSE}
brexit_table <- matrix(c(55, 14, 52, 223), nrow = 2, byrow = TRUE)
dimnames(brexit_table) <- list(c("Cluster 1", "Cluster 2"), c("Remain", "Leave"))
chi_square_result <- chisq.test(brexit_table)
print(chi_square_result)
```

The results from Pearson's Chi-squared test, with Yates' continuity
correction, indicate strong statistical significance regarding the
association between cluster assignments and Brexit vote outcomes. The
test statistic value of 92.339 and a p-value less than 2.2 Ã— 10\^-16
reject the null hypothesis of no association between cluster membership
and how wards voted in the Brexit referendum.

The test implies a statistically significant association between the
cluster a ward belongs to and whether it voted to Leave or Remain in the
EU. This suggests the characteristics that defined each cluster, such as
socio-economic status, median age, education levels, are likely
influential factors determining Brexit voting patterns and predictors of
voting behavior. The Chi-squared test confirms the way wards were
clustered based on demographic and socio-economic data aligns with their
voting behavior, indicating powerful insights into how regional or local
characteristics influenced political decisions in the Brexit referendum.

## Task 2 : Logistic Regression:

Logistic regression is well-suited for analyzing the Brexit data-set and
voting behavior due to the following reasons. The data-set has a binary
outcome (voting Leave or Remain), making logistic regression an
appropriate choice for modeling. Logistic regression provides the
probabilities of outcomes, allowing understanding of voting behaviors
under different conditions. The model coefficients indicate how factors
like age, income, education, and proportion of non-UK-born residents
influence the voting decision. Logistic regression can capture
non-linear relationships between predictors and the probability of the
outcome.

```{r task2 final,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
# Build the logistic regression model

# Load necessary libraries
library(tidyverse)  # for data manipulation and visualization
library(broom)      # for tidying model outputs

# Load the dataset
brexit_data <- read.csv("brexit.csv")

# Ensure that the voteBrexit is a factor
brexit_data$voteBrexit <- as.factor(brexit_data$voteBrexit)

model <- glm(voteBrexit ~ abc1 + notBornUK + medianIncome + medianAge + withHigherEd, 
             family = binomial, data = brexit_data)

# Summarize the model to view coefficients
summary(model)

# Order the input variables based on the absolute value of the z-value of their coefficients
coef_summary <- summary(model)$coefficients
coef_summary <- coef_summary[order(abs(coef_summary[, "z value"]), decreasing = TRUE), ]

# View the ordered coefficients
print(coef_summary)

# Assuming you have a summary of your model called 'model_summary'
model_summary <- summary(model)
```

The logistic regression analysis provides insights on factors
influencing Brexit voting behavior. Higher education levels are
negatively associated with supporting Brexit (coefficient of -26.7443, p
\< 0.0001), while middle to upper social classes (coefficient of
17.5780, p \< 0.0001), older ages (coefficient of 5.9209, p \< 0.0001),
and higher incomes (coefficient of -6.3857, p \< 0.001) show positive
associations. Surprisingly, areas with more residents born outside the
UK (coefficient of 5.6861, p \< 0.01) also had increased Brexit support,
highlighting complex dynamics.

```{r,include=TRUE, echo=FALSE}
# Coefficients table
coef_table <- model_summary$coefficients

# Calculate the 95% confidence intervals
conf_intervals <- confint(model)

# Create a data frame from the coefficients for easier plotting
coef_df <- as.data.frame(coef_table)
coef_df$Variable <- row.names(coef_df)
coef_df$ConfLow <- conf_intervals[,1]
coef_df$ConfHigh <- conf_intervals[,2]
```

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.align='center',fig.width=5,fig.height=3}
# Plot using ggplot2
library(ggplot2)
ggplot(coef_df, aes(x = Variable, y = Estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ConfLow, ymax = ConfHigh), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Logistic Regression Coefficients", y = "Coefficient Value", x = "")
```

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
probs <- predict(model, type = "response")
class.pred <- 1*(probs > 0.627199)
 # Verify predictions
head(cbind(probs,class.pred), 10)
truth.table <- table(brexit_data$voteBrexit, class.pred)
truth.table
 
 #Totalnumberofobservationsintruth.table
N<-sum(truth.table)

 #Misclassificationerror
(truth.table[1,2]+ truth.table[2,1])/N
 
 #Accuracy=Proportionofcorrectpredictions
(truth.table[1,1]+ truth.table[2,2])/N

```

The analysis looks at predicted probabilities, classification decisions,
and a truth table for model performance. Predicted probabilities show
high confidence in wards voting to leave, all surpassing 0.5 threshold.
The truth table has true negatives (82), false positives (25), false
negatives (23), and true positives (214). These metrics give a
misclassification error of 13.95% and accuracy of 86.05%. The model
shows high sensitivity and specificity, effectively identifying both
leave and remain voters, with balanced accuracy avoiding bias.

However, the model has slight bias towards predicting leave outcomes,
with more false positives (25) than false negatives (23). This potential
over-prediction could stem from model parameters or data traits. Still,
the logistic regression model proves valuable for analyzing voting
patterns, offering insights.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,fig.align="center",fig.height=12,fig.width=15,warning=FALSE}

# Load required libraries
library(ggplot2)
library(gridExtra)

# Fit the logistic regression model
brexit_logit_model <- glm(voteBrexit ~ abc1 + medianIncome + medianAge + withHigherEd + notBornUK, 
                          data = brexit_data, family = binomial())

# Create a data frame for predictions
brexit_data_predictions <- data.frame(
  abc1 = brexit_data$abc1,
  medianIncome = brexit_data$medianIncome,
  medianAge = brexit_data$medianAge,
  withHigherEd = brexit_data$withHigherEd,
  notBornUK = brexit_data$notBornUK,
  voteBrexit = brexit_data$voteBrexit 
)

# Add fitted probabilities to the data frame
brexit_data_predictions$fitted_probs <- predict(brexit_logit_model, newdata = brexit_data_predictions, type = "response")

# Function to plot each predictor against fitted probabilities
plot_effect <- function(predictor) {
  ggplot(brexit_data_predictions, aes_string(x = predictor, y = "fitted_probs")) +
    geom_point(aes(color = factor(voteBrexit)), alpha = 0.5) +
    geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "black") +
    labs(title = paste("Effect of", predictor, "on Probability of Voting Leave"),
         x = predictor, y = "Probability of Leave Vote") +
    scale_color_manual(values = c("FALSE" = "blue", "TRUE" = "red"), labels = c("Remain", "Leave")) +
    theme_minimal() +
    theme(legend.title = element_blank(), legend.position = "bottom")
}

# Create plots for each predictor
plot1 <- plot_effect("abc1")
plot2 <- plot_effect("medianIncome")
plot3 <- plot_effect("medianAge")
plot4 <- plot_effect("withHigherEd")
plot5 <- plot_effect("notBornUK")

# Arrange plots in a grid layout with center aligned last graph
grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol = 2)



```

The five graphs show the relationship between socio-economic and
demographic factors and the likelihood of voting to leave the EU.

1.  **Probability of Voting Leave by `abc1`**: Higher social class
    levels correspond to a lower likelihood of voting leave, suggesting
    higher classes had a more cautious attitude towards Brexit's
    economic effects.

2.  **Probability of Voting Leave by `medianIncome`**: Increasing
    incomes correlate with a lower probability of voting leave,
    indicating wealthier areas preferred to remain due to economic
    uncertainty concerns.

3.  **Probability of Voting Leave by `medianAge`**: Older populations
    show a higher probability of voting leave, possibly reflecting
    generational differences in values or national identity views.

4.  **Probability of Voting Leave by `withHigherEd`**: Higher education
    levels are strongly associated with voting remain, suggesting
    educated populations favored staying due to better access to
    information and critical engagement.

5.  **Probability of Voting Leave by `notBornUK`**: Areas with more
    non-UK-born residents were less likely to vote leave, possibly
    reflecting concerns of these communities about maintaining EU ties.

Across graphs, socio-economic stability indicators (higher income,
higher education) align with a Remain vote, while indicators of a
potentially conservative demographic (older age) associate more with a
Leave vote. These insights can help policymakers understand voter
motivations and guide future strategies to address concerns influencing
the Brexit vote.

# Comparing with Guardian Results:

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.height=15,fig.width=15}
# Read the CSV data
data <- read.csv("brexit.csv")

library(ggplot2)
library(scales)
library(dplyr)
library(gridExtra)

# Convert voteBrexit to factor with labels "remain" and "leave"
data$voteBrexit <- factor(data$voteBrexit, levels = c(FALSE, TRUE), labels = c("remain", "leave"))

# Get variable names
variables <- names(data)[1:5]

# Create an empty list to store plots
plots_list <- list()

# Create scatter plots for each variable
for (var in variables) {
  plot_title <- paste("Scatter Plot of", var)
  
  # Calculate percentages
  remain_percent <- round(sum(data$voteBrexit == "remain") / nrow(data) * 100, 2)
  leave_percent <- round(sum(data$voteBrexit == "leave") / nrow(data) * 100, 2)
  
  # Create scatter plot
  plot <- ggplot(data, aes_string(x = "voteBrexit", y = var, color = "voteBrexit")) +
            geom_jitter(height = 0.2, width = 0.05, alpha = 0.5) +
            scale_color_manual(values = c("skyblue", "goldenrod")) +
            labs(x = NULL, y = var) +
            scale_y_continuous(limits = c(0, 1)) +
            scale_x_discrete(labels = c(paste0("remain (", remain_percent, "%)"), paste0("leave (", leave_percent, "%)"))) +
            theme_minimal() +
            theme(legend.position = "none") +
            ggtitle(plot_title) +
            geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50")
  
  # Add plot to the list
  plots_list[[var]] <- plot
}

# Print plots in a grid
grid.arrange(grobs = plots_list, ncol = 2, height = 40, width = 15)

```

The scatter plots generated for each variable versus `VotedExit` showed similar patterns to the results on The Guardian's website. The comparison suggests that `withHigherEd` had the most significant impact, as those with higher education tended to vote for Brexit. However, the analysis approach used is not mentioned, and different methods can lead to varying interpretations. The results alone may not provide a complete understanding of Brexit voting behavior. Other factors like sample selection, variable choices, and model assumptions can influence the findings. Therefore, it is important to consider these results in the broader context, along with external visualizations and explanations, to gain a comprehensive understanding of the factors influencing the Brexit vote. 


## Task 3: "Bagging" in Logistic Regression:

Bagging, or Bootstrap Aggregating, is a powerful ensemble technique that
can enhance the performance of logistic regression models, including for
the Brexit data-set analysis. By averaging multiple logistic regression
models trained on different subsets of the data, bagging reduces
variance and improves accuracy, making the model more stable and less
prone to over-fitting. It can handle imbalanced data by creating
multiple balanced training sets, and deal with model instability caused
by high-dimensional or correlated predictors. Additionally, bagging
introduces a form of non-linearity by aggregating predictions from
multiple models, potentially capturing complex patterns more
effectively. Through the aggregation process, it also provides insights
into the most influential features for predicting the outcome, aiding in
model interpretation.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
# Load necessary library
library(boot)
library(ggplot2)

# Logistic regression model
brexit_logit_model <- glm(voteBrexit ~ abc1 + medianIncome + medianAge + withHigherEd + notBornUK, 
                          data = brexit_data, family = binomial())

# Bootstrap function for logistic regression
boot_func <- function(data, indices) {
  data_boot <- data[indices, ]
  fit <- glm(voteBrexit ~ abc1 + medianIncome + medianAge + withHigherEd + notBornUK, 
             data = data_boot, family = binomial())
  return(coef(fit))
}

# Perform bootstrapping
set.seed(123)
boot_results <- boot(brexit_data, boot_func, R = 1000)

# Display variability results
boot_coefs <- boot_results$t
coefs_variability <- apply(boot_coefs, 2, function(x) c(Mean = mean(x), SD = sd(x)))
print(coefs_variability)
```

### **Analysis of Bootstrap Results:**

The output from the bootstrap analysis provides the mean and standard
deviation (SD) of the logistic regression coefficients for each input
variable: `abc1`, `medianIncome`, `medianAge`, `withHigherEd`,
`notBornUK`, and the Intercept.

**Relevance of Inputs:**

To determine the relevance of each input variable, two factors are
considered: the magnitude of effect (larger absolute values of the
coefficient mean imply a stronger effect on the outcome) and the
stability of effect (lower SD values indicate more stable estimates
across different data samples).

### **Ordering Variables by Relevance:**

1.  **withHigherEd (-27.372175 Mean, 3.894985 SD)**: Despite its higher
    SD, the absolute mean is the largest, indicating a very strong
    negative effect on the likelihood of voting Leave, though slightly
    less stable.

2.  **abc1 (18.119709 Mean, 2.849313 SD)**: Shows a significant positive
    effect with relatively low variability, indicating a strong and
    reliable influence.

3.  **medianIncome (-6.717543 Mean, 1.966824 SD)**: Moderate negative
    effect with moderate stability, suggesting a substantial but less
    pronounced impact than `abc1` or `withHigherEd`.

4.  **medianAge (6.008079 Mean, 1.353211 SD)**: Positive effect with
    relatively low SD, indicating both a low and stable influence.

5.  **notBornUK (5.786003 Mean, 1.800358 SD)**: Similar to `medianAge`
    in terms of effect strength and stability but slightly less
    influential due to its smaller mean.

**Justification:**

The order of relevance is determined primarily by the magnitude of the
effect, as the goal is to identify which variables most significantly
influence the probability of voting Leave. Stability (SD) is considered
to ensure the reliability of the variables' effects across different
samples.

Variables like `withHigherEd` and `abc1`, which show strong effects
(both positive and negative), highlight key socio-economic factors
influencing Brexit votes, aligning with general expectations about
voting demographics.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,fig.align="center"}

boot_func <- function(data, indices) {
  data_boot <- data[indices, ]
  fit <- glm(voteBrexit ~ abc1 + medianIncome + medianAge + withHigherEd + notBornUK, 
             data = data_boot, family = binomial())
  coefs <- coef(fit)
  names(coefs) <- c("Intercept", "abc1", "medianIncome", "medianAge", "withHigherEd", "notBornUK")
  return(coefs)
}

# Perform bootstrapping
set.seed(123)
boot_results <- boot(brexit_data, boot_func, R = 1000)

# Convert boot results to data frame
boot_df <- as.data.frame(boot_results$t)

# Rename columns
colnames(boot_df) <- c("Intercept", "abc1", "medianIncome", "medianAge", "withHigherEd", "notBornUK")

# Reshape data for plotting
library(tidyr)
boot_df_long <- pivot_longer(boot_df, everything(), names_to = "Coefficient", values_to = "Estimate")

# Plotting
library(ggplot2)
ggplot(boot_df_long, aes(x = Estimate, fill = Coefficient)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Coefficient, scales = "free") +
  labs(title = "Density Plot of Bootstrap Coefficient Estimates",
       x = "Coefficient Estimate",
       y = "Density") +
  theme_minimal()


```


## Task 4: Alternate Logistic Regression:


```{r results='asis',include=TRUE, echo=FALSE,error=FALSE,message=FALSE,fig.align="center",warning=FALSE}
library(knitr)

# VIF values
vif_values <- c(abc1 = 9.994053, 
                medianIncome = 2.683698, 
                medianAge = 3.111688, 
                withHigherEd = 8.558226, 
                notBornUK = 3.406493)

# Create a data frame for VIF values
vif_df <- data.frame(Predictor = names(vif_values),
                     VIF = vif_values,
                     Multicollinearity = rep(NA, length(vif_values)))

# Classify multicollinearity levels
vif_df$Multicollinearity[vif_df$VIF > 9.8] <- "High"
vif_df$Multicollinearity[vif_df$VIF <= 10 & vif_df$VIF > 5] <- "Moderate"
vif_df$Multicollinearity[vif_df$VIF <= 5] <- "Low"

# Description
description <- "Variance Inflation Factor (VIF) values measure the degree of multicollinearity in logistic regression predictors. 
A higher VIF indicates stronger multicollinearity, which can affect the reliability of coefficient estimates. 
- High: VIF > 10 (Strong multicollinearity)
- Moderate: 5 <= VIF <= 10 (Moderate multicollinearity)
- Low: VIF < 5 (Low multicollinearity)"

# Print the table with description
cat(paste0("### Description:\n", description, "\n\n"))
kable(vif_df, caption = "VIF Values for Logistic Regression Predictors")

```

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,fig.align="center",warning=FALSE}
library(car)

# Calculate VIF for predictors in logistic regression
vif_values <- vif(brexit_logit_model)

# Print VIF values
print(vif_values)
```

### Lasso Regression (L1):

To enhance the analysis for Task 2, LASSO (Least Absolute Shrinkage and
Selection Operator) regression will be used as an alternative approach.
LASSO regression introduces a penalty term to the loss function,
proportional to the absolute value of the coefficients. This method
performs variable selection and regularization together, helping to
manage multicollinearity and enhance model interpretation by shrinking
some coefficients to zero.

```{r ,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE,fig.align="center"}
# Load necessary library
library(glmnet)

# Prepare the matrix of predictors and the response variable
x <- as.matrix(brexit_data[, c("abc1", "medianIncome", "medianAge", "withHigherEd", "notBornUK")])
y <- brexit_data$voteBrexit

cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)
# Choose the lambda that minimizes the cross-validation error
optimal_lambda <- cv_lasso$lambda.min
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = optimal_lambda)

# Display the coefficients
coef(lasso_model)
```

```{r,fig.cap="LASSO Regression",include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
# Perform cross-validation
cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# Obtain optimal lambda value
lambda_optimal <- cv_lasso$lambda.min

# Print the optimal lambda value
print(paste("Optimal lambda:", lambda_optimal))

plot(cv_lasso)
```

**Coefficient Statistics from LASSO Model:**

The coefficients listed are for the LASSO model at a specific value of
Lambda:

-   Baseline log-odds of 0.1136345 for voting to leave when all
    predictors are zero.

-   Positive coefficient (16.1900907) for `abc1` variable, suggesting
    higher social/economic status increases log-odds of voting to leave.

-   Negative coefficient (-5.7990745) for median income, indicating
    higher incomes decrease likelihood of voting to leave.

-   Positive coefficient (5.4011546) for median age, with older ages
    increasing likelihood of voting to leave.

-   Large negative coefficient (-25.0582764) for higher education
    levels, significantly reducing likelihood of voting to leave.

-   Positive coefficient (4.9703831) for proportion of non-UK born
    residents, increasing likelihood of voting to leave.

The LASSO model coefficients at the selected lambda value provide
insights into variable importance, with `withHigherEd` and `abc1`
exhibiting the strongest effects on influencing the leave decision.
LASSO helps create simpler, more interpretable models by penalizing
coefficient magnitudes, avoiding over-fitting. Understanding which
demographics are more likely to vote in certain ways based on factors
like education levels and social status can aid in targeted campaigns
and policy-making strategies.

## Ridge Regression(L2):

To cross check with the LASSO regression, Ridge (L2) regularization is
also used.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
library(glmnet)

# Perform cross-validation for ridge regression
cv_ridge <- cv.glmnet(x, y, family = "binomial", alpha = 0)

# Obtain optimal lambda value
lambda_optimal <- cv_ridge$lambda.min

# Fit ridge regression model with optimal lambda
ridge_model <- glmnet(x, y, family = "binomial", alpha = 0, lambda = lambda_optimal)

# Print coefficients of the ridge regression model
print(coef(ridge_model))


```

The final coefficients provided are calculated at one of these lambda
points:

-   Baseline log-odds of 2.0967654 when all predictors are zero.

-   Positive relationship between `abc1` variable (2.0129291) and the
    outcome probability.

-   Negative relationship with median income (-1.5999134), higher
    incomes decrease outcome probability.

-   Positive relationship with median age (1.5999171), older ages
    increase outcome likelihood.

-   Significant negative impact of higher education levels (-7.3779121),
    strongly decreasing outcome probability.

-   Slight negative relationship (-0.4333877) where more non-UK born
    residents slightly lower outcome probability.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE,warning=FALSE}
# Fit Ridge Regression model
cv_ridge <- cv.glmnet(x, y, family = "binomial", alpha = 0)
plot(cv_ridge)
```

The information includes a plot showing the regularization path for a
Ridge Regression model and the final coefficients at a specific lambda
value. Ridge Regression analyzes data with potentially multicollinear
variables by imposing a penalty, shrinking coefficients towards zero.
The plot shows how coefficients evolve as the penalty term lambda
changes. Vertical dotted lines likely represent lambda values used to
select model complexity. The final coefficients are calculated at one of
these lambda points.

Ridge Regression helps prevent over-fitting by adding a penalty for
large coefficients. This makes the model more stable and better at
predicting new data, especially when there are many predictors or they
are highly correlated. Choosing the right value for lambda is important,
as it controls the model complexity. This is typically done using
cross-validation to strike a balance between under-fitting and
over-fitting. The coefficients at the chosen lambda value show which
factors have the biggest impact on the outcome, positive or negative.


Below are the comparison of all three regression models.

```{r,include=TRUE, echo=FALSE,error=FALSE,message=FALSE, fig.align="center",warning=FALSE}
# Load necessary libraries
library(glmnet)
library(ggplot2)
library(dplyr)


# Prepare the matrix of predictors and the response variable
x <- as.matrix(brexit_data[, c("abc1", "medianIncome", "medianAge", "withHigherEd", "notBornUK")])
y <- as.factor(brexit_data$voteBrexit)

# Fit Standard Logistic Regression
std_model <- glm(voteBrexit ~ abc1 + medianIncome + medianAge + withHigherEd + notBornUK, 
                 data = brexit_data, family = binomial())

# Fit LASSO and Ridge models using glmnet
set.seed(123)
cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)  # LASSO
cv_ridge <- cv.glmnet(x, y, family = "binomial", alpha = 0)  # Ridge

# Predict probabilities from each model
brexit_data$Prob_Std <- predict(std_model, newdata = brexit_data, type = "response")
brexit_data$Prob_Lasso <- predict(cv_lasso, newx = x, s = "lambda.min", type = "response")
brexit_data$Prob_Ridge <- predict(cv_ridge, newx = x, s = "lambda.min", type = "response")

# Function to create comparison plots for each predictor
plot_comparison <- function(predictor) {
  ggplot(brexit_data, aes_string(x = predictor)) +
    geom_point(aes(y = Prob_Std, color = "Logistic"), alpha = 0.4, size = 2) +
    geom_point(aes(y = Prob_Lasso, color = "LASSO"), alpha = 0.4, size = 2) +
    geom_point(aes(y = Prob_Ridge, color = "Ridge"), alpha = 0.4, size = 2) +
    geom_smooth(aes(y = Prob_Std, color = "Standard"), method = "glm", method.args = list(family = "binomial"), se = FALSE) +
    geom_smooth(aes(y = Prob_Lasso, color = "LASSO"), method = "glm", method.args = list(family = "binomial"), se = FALSE) +
    geom_smooth(aes(y = Prob_Ridge, color = "Ridge"), method = "glm", method.args = list(family = "binomial"), se = FALSE) +
    scale_color_manual(values = c("Logistic" = 'violet', "LASSO" = 'blue', "Ridge" = 'green')) +
    labs(title = paste("Comparison of Logistic, LASSO, and Ridge Models for", predictor),
         x = predictor, y = "Probability of Leave Vote") +
    theme_minimal() +
    theme(legend.title = element_blank(), legend.position = "top")
}

# Create plots for each predictor
plot_comparison("abc1")
plot_comparison("medianIncome")
plot_comparison("medianAge")
plot_comparison("withHigherEd")
plot_comparison("notBornUK")


```

# Conclusion:

The analysis of Brexit voting data shows education levels strongly impacted voting Leave. Higher education decreased the chance of voting Leave, while higher social status and older age increased the chance. K-means clustering found two distinct groups that matched voting patterns, supported by a significant Chi-squared test. Logistic regression models like Lasso and Ridge consistently identified education and social status as key factors. Bagging confirmed these findings were stable. These insights suggest improving education and economic conditions could influence future political decisions. This provides guidance for understanding and addressing voter motivations for policymakers and strategists.

